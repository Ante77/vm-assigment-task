{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3716c02-ac39-47e2-a7e0-5652999e09b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Antisha\\.cache\\kagglehub\\datasets\\mateibejan\\15000-gutenberg-books\\versions\\5\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"mateibejan/15000-gutenberg-books\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65558e68-7cc5-4804-8431-bcd3a2322dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't acquire text for Ragged Dick with ID 20689. Link: http://www.gutenberg.org/ebooks/20689\n",
      "Couldn't acquire text for The Owl and the Pussycat with ID 23897. Link: http://www.gutenberg.org/ebooks/23897\n",
      "Couldn't acquire text for Don Quixote, Volume 1 with ID 28842. Link: http://www.gutenberg.org/ebooks/28842\n",
      "Couldn't acquire text for The Gift of the Magi with ID 22440. Link: http://www.gutenberg.org/ebooks/22440\n",
      "Couldn't acquire text for The Garden Party, and Other Stories with ID 26463. Link: http://www.gutenberg.org/ebooks/26463\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from gutenbergpy.gutenbergcache import GutenbergCache\n",
    "from gutenbergpy.textget import get_text_by_id\n",
    "\n",
    "PATH_TO_METADATA = r\"C://Users//Antisha//.cache//kagglehub//datasets//mateibejan//15000-gutenberg-books//versions//5//gutenberg_metadata.csv\"\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_listed_text = []\n",
    "    listed_text = list(text)\n",
    "\n",
    "    for iter in range(len(listed_text) - 1):\n",
    "        if (listed_text[iter] == '\\\\' and listed_text[iter + 1] == 'n') or \\\n",
    "            (listed_text[iter] == 'n' and listed_text[iter - 1] == '\\\\'):\n",
    "            continue\n",
    "        elif listed_text[iter] == '\\\\' and listed_text[iter + 1] == 'r' or \\\n",
    "            (listed_text[iter] == 'r' and listed_text[iter - 1] == '\\\\'):\n",
    "            continue\n",
    "        elif listed_text[iter] == '\\\\' and listed_text[iter + 1] == 't' or \\\n",
    "            (listed_text[iter] == 't' and listed_text[iter - 1] == '\\\\'):\n",
    "            continue\n",
    "        elif listed_text[iter] == '\\\\':\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_listed_text.append(listed_text[iter])\n",
    "\n",
    "    cleaned_text = ''.join([str(char) for char in cleaned_listed_text])\n",
    "    return cleaned_text\n",
    "\n",
    "df_metadata = pd.read_csv(PATH_TO_METADATA)\n",
    "\n",
    "data = {'Author': None, 'Title': None, 'Link': None, 'ID': None, 'Bookshelf': None, 'Text': None}\n",
    "\n",
    "\n",
    "for key, row in df_metadata.iterrows():\n",
    "    if data['Author'] == None:\n",
    "        data['Author'] = [row['Author']]\n",
    "    else:\n",
    "        data['Author'].append(row['Author'])\n",
    "    \n",
    "    if data['Title'] == None:\n",
    "        data['Title'] = [row['Title']]\n",
    "    else:\n",
    "        data['Title'].append(row['Title'])\n",
    "    \n",
    "    if data['Link'] == None:\n",
    "        data['Link'] = [row['Link']]\n",
    "    else:\n",
    "        data['Link'].append(row['Link'])\n",
    "    \n",
    "    book_id = int(row['Link'].split('/')[-1])\n",
    "\n",
    "    if data['ID'] == None:\n",
    "        data['ID'] = [book_id]\n",
    "    else:\n",
    "        data['ID'].append(book_id)\n",
    "    \n",
    "    if data['Bookshelf'] == None:\n",
    "        data['Bookshelf'] = [row['Bookshelf']]\n",
    "    else:\n",
    "        data['Bookshelf'].append(row['Bookshelf'])\n",
    "\n",
    "    try:\n",
    "        text = get_text_by_id(book_id)\n",
    "        text = clean_text(str(text))\n",
    "    except:\n",
    "        try: \n",
    "            page = requests.get(row['Link'])\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            text_link = 'http://www.gutenberg.org' + soup.find_all(\"a\", string=\"Plain Text UTF-8\")[0]['href']\n",
    "            http_response_object = urlopen(text_link)\n",
    "\n",
    "            text = http_response_object.read().decode('utf-8')\n",
    "            text = clean_text(str(text))\n",
    "        except:\n",
    "            print(\"Couldn't acquire text for \" + row['Title'] + ' with ID ' + str(book_id) + '. Link: ' + row['Link'])\n",
    "            \n",
    "    if data['Text'] == None:\n",
    "        data['Text'] = [' '.join(text.split(' '))]\n",
    "    else:\n",
    "        try:\n",
    "            data['Text'].append(' '.join(text.split(' ')))\n",
    "        except:\n",
    "            data['Text'].append(None)\n",
    "            print(\"Couldn't save data for \" + row['Title'] + ' with ID ' + str(book_id) + '. Link: ' + row['Link'])\n",
    "\n",
    "df_data = pd.DataFrame(data, columns = ['Title', 'Author', 'Link', 'ID', 'Bookshelf', 'Text'])\n",
    "df_data.to_csv(os.path.join(os.getcwd(), 'data', 'gutenberg_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cde38-318b-4c76-847f-5525e66b4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
